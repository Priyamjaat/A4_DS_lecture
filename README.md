Assignment 4 â€“ Density Estimation using GAN (Deep Learning)

This assignment demonstrates how to estimate the probability density function (PDF) of a transformed dataset using a Generative Adversarial Network (GAN) implemented in PyTorch.

The project involves data preprocessing, GAN training, synthetic data generation, and density estimation using Kernel Density Estimation (KDE).

ğŸš€ Project Overview

The goal of this assignment is to:

Load and preprocess real-world data (no2 column from dataset)

Apply a nonlinear transformation to the data

Normalize the transformed data

Train a GAN (Generator + Discriminator)

Generate synthetic samples

Compare real vs generated distributions

Estimate the final probability density using KDE

ğŸ› ï¸ Technologies & Libraries Used

Python

NumPy â€“ Numerical computations

Pandas â€“ Data handling

Matplotlib â€“ Visualization

Scikit-learn â€“ Kernel Density Estimation

PyTorch â€“ GAN implementation (Deep Learning)

ğŸ“‚ Project Workflow
1ï¸âƒ£ Data Loading
df = pd.read_csv("data.csv", encoding='latin1')
x = df["no2"].dropna().values.astype(np.float32)


Extracts no2 column

Removes missing values

Converts to float format

2ï¸âƒ£ Nonlinear Transformation

The data is transformed using:

ğ‘§
=
ğ‘¥
+
ğ‘
ğ‘Ÿ
â‹…
sin
â¡
(
ğ‘
ğ‘Ÿ
â‹…
ğ‘¥
)
z=x+a
r
	â€‹

â‹…sin(b
r
	â€‹

â‹…x)

Where:

a_r and b_r are computed using roll number

This creates a modified nonlinear dataset

3ï¸âƒ£ Data Normalization
z_norm = (z - z_mean) / z_std


Normalization ensures stable GAN training.

ğŸ¤– GAN Architecture
Generator

Input: Random noise (1D)

Architecture:

Linear(1 â†’ 32)

ReLU

Linear(32 â†’ 32)

ReLU

Linear(32 â†’ 1)

Discriminator

Input: Real or Fake data

Architecture:

Linear(1 â†’ 32)

LeakyReLU

Linear(32 â†’ 32)

LeakyReLU

Linear(32 â†’ 1)

Sigmoid

âš™ï¸ Training Details

Loss Function: Binary Cross Entropy (BCELoss)

Optimizer: Adam

Learning Rate: 0.0002

Epochs: 4000

Batch Size: 128

Device: GPU (if available) else CPU

Training process:

Train Discriminator on real & fake data

Train Generator to fool Discriminator

Repeat for multiple epochs

ğŸ“ˆ Results & Visualization
1ï¸âƒ£ Histogram Comparison

Real transformed data distribution

GAN generated data distribution

Visual comparison of PDFs

plt.hist(z, bins=80, density=True)
plt.hist(gen_z, bins=80, density=True)

2ï¸âƒ£ Kernel Density Estimation (KDE)

After generating 10,000 samples:

kde = KernelDensity(kernel='gaussian', bandwidth=0.3).fit(gen_z)


Estimated final probability density

Smooth PDF curve plotted

ğŸ“Š Final Output

âœ”ï¸ GAN successfully learns the transformed data distribution
âœ”ï¸ Generated samples closely match real distribution
âœ”ï¸ KDE provides smooth density estimation

ğŸ¯ Learning Outcomes

Understanding GAN architecture

Implementing Generator & Discriminator in PyTorch

Working with adversarial training

Performing density estimation using KDE

Comparing real vs synthetic distributions

ğŸ‘¨â€ğŸ’» Author

Priyam Chaudhary
B.E. Computer Science & Engineering
